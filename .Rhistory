iq1 <- c(100, 110, 140, 95, 99)
iq1
seq <- 1:5
seq
iq1+iq2
iq1+seq
iq1 > seq
iq1 == seq
df <- data.frame(iq1, seq)
df
df$seq
df[2,2]
addict <- read.csv("internet.csv")
library(readxl)
Data_Market_Category_20190106 <- read_excel("F:/2019 겨울방학/AoM Submission 2019/Market Category Paper/Data, Log, & Result Files/Data_Market Category_20190106.xlsx")
View(Data_Market_Category_20190106)
View(Data_Market_Category_20190106)
View(Data_Market_Category_20190106)
summay(Year)
describe(Year)
installed.packages(describe)
install.packages(describe)
install.packages("psych")
library psych
library(psych)
describe(CAR_3day)
describe(Data_Market_Category_20190106$centered_DJSI)
describe(Data_Market_Category_20190106$CAR_3day)
install.packages("psych")
packages = c("tm", "KoNLP", "topicmodels", "wordcloud", "ggplot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
for(i in packages){}
for(i in packages){if (! require(i, character.only=TRUE)) {install.packages(i, dependencies=TRUE)}
packages = c('tm', 'KoNLP', 'NLP', 'topicmodels', 'wordcloud', 'ggplot2', 'lsa', 'GPArotation', 'RWeka', 'stringr', 'readtext')
if(! require(i, character.only = TRUE))
{install.packages(i, dependencies = TRUE)}
for(i in packages){
for(i in packages){if(! require(i, character.only = TRUE))
for(i in packages){
for(i in packages){
{install.packages(i, dependencies = TRUE)}}
pal <- brewer.pal(9, "set1")
getwd()
getwd()
packages = c("tm", "KoNLP", "NLP", "topicmodels", "wordcloud", "ggplot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
for(i in packages){
if(! require(i, character.only = TRUE))
{install.packages(i, dependencies = TRUE)}
}
getwd()
setwd(F://박사논문 준비 & 작성/박사논문 - Reading Journals)
setwd("F://박사논문 준비 & 작성/박사논문 - Reading Journals")
folder = "./Technology"
text_r <- readtext(paste0(folder, "/*.pdf"))
rm(list = ls())
pal <-brewer.pal(9, "Set1")
folder = "F://박사논문 준비 & 작성/박사논문 - Reading Journals"
text_r <- readtext(paste0(folder, "/*.pdf"))
getwd()
folder = "./Technology"
text_r <- readtext(paste0(folder, "/*.pdf"))
txt = text_r$text
ttt <- array(data = NA, dim = length(txt))
for(i in 1:length(txt)){
t1 <- txt[i]
t2 <- as.String(t1)
t3 <- gsub("\n", " ", t2)
ttt[i] <- t3
}
txt <- ttt
txt<-str_replace_all(txt, "@[[:graph:]]*", "")
txt <- str_replace_all(txt, "http://[[:graph:]]*", "")
txt <- str_replace_all(txt, "[^[:graph:]]", " ")
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("\n", " ", txt)
txt <- gsub("#", " ", txt)
txt <- gsub("\r", " ", txt)
txt <- gsub("RT", " ", txt)
txt <- gsub("http", " ", txt)
txt <- gsub("  ", " ", txt)
corpus <- Corpus(VectorSource(txt))
myStopwords <- c(stopwords("english"))
corpus <- tm_map(corpus, removeWords, myStopwords)
corpus <- tm_map(corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
uniTokenizer <- function(x) unlist(strsplit(as.character(x), "[[:space:]]+"))
control = list(tokenize = words, removeNumbers = TRUE, removePunctuation = TRUE, stopwords = TRUE)
tdm <- DocumentTermMatrix(corpus, control = control)
View(tdm)
## p. 172 빈도표 계산 및 말뭉치 등을 활용한 시각화
word.freq <- apply(tdm.e[,], 2, sum)
word.freq <- apply(tdm[,], 2, sum)
length(word.freq)
head(word.freq)
sort.word.freq <- sort(word.freq, decreasing = TRUE)
sort.word.freq[1:20]
cumsum.word.freq <- cumsum(sort.word.freq)
cumsum.word.freq[1:20]
prop.word.freq <- cumsum.word.freq/cumsum.word.freq[length(cumsum.word.freq)]
prop.word.freq[1:20]
library('wordcloud')
wordcloud(names(word.freq), freq=word.freq, scale = c(4, 0.2), rot.per = 0.0, min.freq=50, random.order = FALSE)
library("RColorBrewer")
pal <- brewer.pal(4, "Dark2")
wordcloud(names(word.freq), freq=word.freq, scale = c(4, 0.2), rot.per=0.0, freq=200, random.order = FALSE, col=pal)
wordcloud(names(word.freq), freq=word.freq, scale = c(4, 0.2), rot.per=0.0, min.freq=5, random.order = FALSE, col=pal)
findAssocs(tdm, "technolog", 0.50)
var1 <- as.vector(tdm[, "technolg"])
var1 <- as.vector(tdm, "technolog")
var1 <- as.vector(tdm[, "technolog"])
var2 <- as.vector(tdm[, "new"])
cor.test(var1, var2)
length.doc <- length(rownames(tdm))
my.doc.cor <- matrix(NA, nrow = length.doc, ncol = length.doc)
for (i in 1:length.doc){
for (j in 1:length.doc){
my.doc.cor[i,j] <- my.assoc.func(tdm, rownames(tdm)[i], rownames(tdm)[j])$est}
}
my.assoc.func <- function(mydtm, term1, term2){
myvar1 <- as.vector(tdm[, term1])
myvar2 <- as.vector(tdm[, term2])
cor.test(myvar1, myvar2)
}
for (i in 1:length.doc){
for (j in 1:length.doc){
my.doc.cor[i,j] <- my.assoc.func(tdm, rownames(tdm)[i], rownames(tdm)[j]$est}}
for (i in 1:length.doc){
for (j in 1:length.doc){
my.doc.cor[i,j] <- my.assoc.func(tdm, rownames(tdm)[i], rownames(tdm)[j])$est
}
}
library(tm)
library(slam)
View(corpus)
tdm1 = "Gavetti & Levinthal. 2000. ASQ - Looking Backward & Forward.pdf"
corpus[[1]]
corpus[[3]]
tdm1 <- corpus[[1]]
tdm2 <- corpus[[2]]
cosine_sim <- tcrossprod_simple_triplet_matrix(dtm1, dtm2)/sqrt(row_sums(dtm1^2) %*% t(row_sums(dtm2^2)))
cosine_sim <- tcrossprod_simple_triplet_matrix(tdm1, tdm2)/sqrt(row_sums(tdm1^2) %*% t(row_sums(tdm2^2)))
tdm <- as.matrix(tdm)
require(proxy)
cosine_dist_mat <- as.matrix(dist(t(tdm), method = "cosine"))
library(proxy)
install.packages(proxy)
"proxy"
install.packages("proxy")
tdm <- as.matrix(tdm)
require(proxy)
cosine_dist_mat <- as.matrix(dist(t(tdm), method = "cosine"))
install.packages("slam")
install.packages("slam")
packages = c("tm", "KoNLP", "NLP", "topicmodels", "wordcloud", "ggplot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
for(i in packages){
+ if(! require(i, character.only = TRUE))
+ {install.packages(i, dependencies = TRUE)}
+ }
folder = "./Technology"
text_r <- readtext(paste0(folder, "/*.pdf"))
packages = c("tm", "KoNLP, "NLP", "topicmodels", "wordcloud", "ggplot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
packages = c("tm", "KoNLP", "NLP", "topicmodels", "wordcloud", "ggplot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
for(i in packages){
if(! require(i, character.only=TRUE))
{install.packages(i, dependencies = TRUE)}
}
rm(list = ls())
folder = "./Technology"
text_r <- readtext(paste0(folder, "/*.pdf"))
txt = text_r$text
ttt <- array(data=NA, dim=length(txt))
for(i in 1:length(txt)){
t1 <- txt[i]
t2 <- as.String(t1)
t3 <- gsub("\n", " ", t2)
ttt[i] <- t3
}
txt <- ttt
txt <- str_replace_all(txt, "@[[:graph:]]*", "")
txt <- str_replace_all(txt, "http://[[:grpah:]]*", "")
txt <- str_replace_all(txt, "http://[[:graph:]]", "")
txt <- str_replace_all(txt, "[^[:graph:]]", " ")
txt <- gsub("[[:puct:]]", "", txt)
txt <- gsub("[[punct:]]", "", txt)
txt <- gsub("\n", " ", txt)
txt <- gsub("#", " ", txt)
txt <- gsub("\r", " ", txt)
txt <- gsub("RT", " ", txt)
txt <- gsub("http", " ", txt)
txt <- gsub("  ", " ", txt)
corpus <- Corpus(VectorSource((txt))
)
myStopwords = c(stopwords("english"))
corpus <- tm_map(corpus, removeWords, myStopwords)
corpus<- tm_map(corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
uniTokenizer <- function(x) unlist(strsplit(as.character(x), "[[:space:]]+"))
control = list(tokenize = words, removeNumbers = T, removePunctuation = T, stopwords = T)
tdm <- DocumentTermMatrix(corpus, control = control)
library(slam)
cosine_dist_mat <- 1 - crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
tdm1 <- corpus[[1]]
View(tdm1)
tdm2 <- corpus[[3]]
cosine_sim <- tcrossprod_simple_triplet_matrix(tdm1, tdm2)/sqrt(row_sums(tdm1^2) %*% t(row_sums(tdm2^2)))
View(tdm)
tdm1 <- tdm[1:5]
tdm1 <- tdm$nrow[1:5]
require(proxy)
cosine_dist_mat <- as.matrix(dist(t(tdm), method = "cosine"))
tdm1 <- as.matrix(tdm1)
cosine_dist_mat <- as.matrix(dist(t(tdm1), method = "cosine"))
diag(cosine_dist_mat) <- NA
cosine_dist <- apply(cosine_dist_mat, 2, mean, na.rm=TRUE)
cosine_dist
inspect(tdm)
tdm <- as.matrix(tdm)
install.packages("proxy")
library(proxy)
cosine_dist_mat <- as.matrix(dist(t(tdm), method = "cosine"))
diag(cosine_dist_mat) <- NA
cosine_dist <- apply(cosine_dist_mat, 2, mean, na.rm=TRUE)
cosine_dist
ave(cosine_dist)
