{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Obama_2016.txt', 'r') as f:\n",
    "    doc = f.read()\n",
    "    \n",
    "# 읽고자하는 txt파일이 정확히 Jupyter File이 있는 폴더에 \"함께\" 들어있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"anuary 12, 2016\\nThank you. Mr. Speaker, Mr. Vice President, Members of Congress, my fellow Americans: Tonight marks the eighth year that I've come here to report on the State of the Union. And for this final one, I'm going to try to make it a little shorter. I know some of you are antsy to get back to Iowa. [Laughter] I've been there. I'll be shaking hands afterwards if you want some tips. [Laughter]\\n\\nNow, I understand that because it's an election season, expectations for what we will achieve t\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anuary',\n",
       " '12',\n",
       " ',',\n",
       " '2016',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Speaker',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Vice',\n",
       " 'President',\n",
       " ',',\n",
       " 'Members',\n",
       " 'of',\n",
       " 'Congress',\n",
       " ',',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'Americans',\n",
       " ':',\n",
       " 'Tonight',\n",
       " 'marks',\n",
       " 'the',\n",
       " 'eighth',\n",
       " 'year',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'come',\n",
       " 'here',\n",
       " 'to',\n",
       " 'report',\n",
       " 'on',\n",
       " 'the',\n",
       " 'State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " '.',\n",
       " 'And',\n",
       " 'for',\n",
       " 'this',\n",
       " 'final',\n",
       " 'one',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "\n",
    "doc_tokens = nltk.word_tokenize(doc)\n",
    "doc_tokens[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anuary',\n",
       " '12',\n",
       " ',',\n",
       " '2016',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Speaker',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Vice',\n",
       " 'President',\n",
       " ',',\n",
       " 'Members',\n",
       " 'of',\n",
       " 'Congress',\n",
       " ',',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'Americans',\n",
       " ':',\n",
       " 'Tonight',\n",
       " 'mark',\n",
       " 'the',\n",
       " 'eighth',\n",
       " 'year',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'come',\n",
       " 'here',\n",
       " 'to',\n",
       " 'report',\n",
       " 'on',\n",
       " 'the',\n",
       " 'State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " '.',\n",
       " 'And',\n",
       " 'for',\n",
       " 'this',\n",
       " 'final',\n",
       " 'one',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "\n",
    "doc_lemma = []\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "for token in doc_tokens:\n",
    "    doc_lemma.append(lemma.lemmatize(token))\n",
    "doc_lemma[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anuary', 'JJ'),\n",
       " ('12', 'CD'),\n",
       " (',', ','),\n",
       " ('2016', 'CD'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Speaker', 'NNP'),\n",
       " (',', ','),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Vice', 'NNP'),\n",
       " ('President', 'NNP'),\n",
       " (',', ','),\n",
       " ('Members', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Congress', 'NNP'),\n",
       " (',', ','),\n",
       " ('my', 'PRP$'),\n",
       " ('fellow', 'JJ')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lemma_tagged = nltk.pos_tag(doc_lemma)\n",
    "doc_lemma_tagged[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anuary',\n",
       " '2016',\n",
       " 'Thank',\n",
       " 'Speaker',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'Members',\n",
       " 'Congress',\n",
       " 'fellow',\n",
       " 'Americans',\n",
       " 'Tonight',\n",
       " 'mark',\n",
       " 'eighth',\n",
       " 'year',\n",
       " 'come',\n",
       " 'report',\n",
       " 'State',\n",
       " 'Union',\n",
       " 'final',\n",
       " 'going']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([',', '.', '?', '!', ':', ';', '[', ']'])\n",
    "doc_filtered = [word for word in doc_lemma if word not in stop_words and len(word) > 3]\n",
    "doc_filtered[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restart\n",
    "\n",
    "# Building a custom corpus\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "corpusdir = 'APPLE INC_2011-2016/'\n",
    "if not os.path.isdir(corpusdir):\n",
    "    os.mkdir(corpusdir)\n",
    "    \n",
    "corpus = ['APPLE INC_2011.txt', 'APPLE INC_2012.txt', 'APPLE INC_2013.txt', 'APPLE INC_2014.txt', 'APPLE INC_2015.txt', 'APPLE INC_2016.txt' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    doc_tokens = nltk.word_tokenize(doc)\n",
    "    \n",
    "doc_lemma = []\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "for token in doc_tokens:\n",
    "    doc_lemma.append(lemma.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([',', '.', '?', '!', ':', ';', '[', ']'])\n",
    "filtered_doc = [word for word in doc_lemma if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object simple_tokenize at 0x000002AB46892678>, <generator object simple_tokenize at 0x000002AB468C6780>, <generator object simple_tokenize at 0x000002AB468C67D8>, <generator object simple_tokenize at 0x000002AB468C6888>, <generator object simple_tokenize at 0x000002AB468C6308>, <generator object simple_tokenize at 0x000002AB468C6678>]\n",
      "Dictionary(3 unique tokens: ['APPLE', 'INC_', 'txt'])\n",
      "TfidfModel(num_docs=6, num_nnz=18)\n",
      "[[], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "corpus = [tokenize(doc) for doc in corpus]\n",
    "lexicon = gensim.corpora.Dictionary(corpus)\n",
    "tfidf = gensim.models.TfidfModel(dictionary = lexicon, normalize = True)\n",
    "vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "print(corpus)\n",
    "print(lexicon)\n",
    "print(tfidf)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restart 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def make_corpus():\n",
    "    corpus = [x for x in os.listdir('C:/Users/안신혜/★ Text Analysis Study/Jupyter Notebooks for Exercise/APPLE INC_2011-2016') if x.endswith(\"*.txt\")]\n",
    "    for doc in corpus:\n",
    "        filePath= 'C:/Users/안신혜/★ Text Analysis Study/Jupyter Notebooks for Exercise/APPLE INC_2011-2016' + os.path.splitext(file)[0] + \".txt\"\n",
    "        with open(filePath, 'r') as infile:\n",
    "            content = infile.read()\n",
    "            yield content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english',use_idf=True, max_df=0.7, smooth_idf=True)\n",
    "# vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Dictionary(0 unique tokens: [])\n",
      "TfidfModel(num_docs=0, num_nnz=0)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "corpus = make_corpus()\n",
    "\n",
    "corpus = [tokenize(doc) for doc in corpus]\n",
    "lexicon = gensim.corpora.Dictionary(corpus)\n",
    "tfidf = gensim.models.TfidfModel(dictionary = lexicon, normalize = True)\n",
    "vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "print(corpus)\n",
    "print(lexicon)\n",
    "print(tfidf)\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restart 3\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "DOC_PATTERN = r'(?!\\.)[\\w_\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "CAT_PATTERN = r'([\\w_\\s]+)\\.*'\n",
    "\n",
    "corpus = PlaintextCorpusReader('C:/Users/안신혜/★ Text Analysis Study\\Jupyter Notebooks for Exercise/APPLE INC_2011-2016', DOC_PATTERN, CAT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PlaintextCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-e15873563a18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'PlaintextCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "corpus = [tokenize(doc) for doc in corpus]\n",
    "lexicon = gensim.corpora.Dictionary(corpus)\n",
    "tfidf = gensim.models.TfidfModel(dictionary = lexicon, normalize = True)\n",
    "vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "print(corpus)\n",
    "print(lexicon)\n",
    "print(tfidf)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restart 4\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "filepath = 'C:/Users/안신혜/★ Text Analysis Study\\Jupyter Notebooks for Exercise/APPLE INC_2011-2016'\n",
    "corpus = PlaintextCorpusReader(filepath,'.*', encoding='utf-8')\n",
    "fids = corpus.fileids()\n",
    "docs = [corpus.words(f) for f in fids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa1 in position 18: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-ee61536b1945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-128-ee61536b1945>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# Get everything we can from this piece.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_tok\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[0;32m    298\u001b[0m                 \u001b[1;34m'block reader %s() should return list or tuple.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m_read_word_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Read 20 lines at a time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mstartpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m             \u001b[0mnew_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[1;31m# If we're at a '\\r', then read one extra character, since\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[1;31m# Decode the bytes into unicode characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m         \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_incr_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \u001b[1;31m# If we got bytes but couldn't decode any, then read further.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_incr_decode\u001b[1;34m(self, bytes)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1434\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m                 \u001b[1;31m# If the exception occurs at the end of the string,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\안신혜\\appdata\\local\\programs\\python\\python36\\lib\\encodings\\utf_8.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa1 in position 18: invalid start byte"
     ]
    }
   ],
   "source": [
    "corpus = [tokenize(doc) for doc in corpus.words(fids)]\n",
    "lexicon = gensim.corpora.Dictionary(corpus)\n",
    "tfidf = gensim.models.TfidfModel(dictionary = lexicon, normalize = True)\n",
    "vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "print(corpus)\n",
    "print(lexicon)\n",
    "print(tfidf)\n",
    "print(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
