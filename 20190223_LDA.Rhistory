iq1 <- c(100, 110, 140, 95, 99)
iq1
seq <- 1:5
seq
iq1+iq2
iq1+seq
iq1 > seq
iq1 == seq
df <- data.frame(iq1, seq)
df
df$seq
df[2,2]
addict <- read.csv("internet.csv")
library(readxl)
Data_Market_Category_20190106 <- read_excel("F:/2019 겨울방학/AoM Submission 2019/Market Category Paper/Data, Log, & Result Files/Data_Market Category_20190106.xlsx")
View(Data_Market_Category_20190106)
View(Data_Market_Category_20190106)
View(Data_Market_Category_20190106)
summay(Year)
describe(Year)
installed.packages(describe)
install.packages(describe)
install.packages("psych")
library psych
library(psych)
describe(CAR_3day)
describe(Data_Market_Category_20190106$centered_DJSI)
describe(Data_Market_Category_20190106$CAR_3day)
install.packages("psych")
packages = c("tm", "KoNLP", "topicmodels", "wordcloud", "ggplot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
for(i in packages){}
for(i in packages){if (! require(i, character.only=TRUE)) {install.packages(i, dependencies=TRUE)}
packages = c('tm', 'KoNLP', 'NLP', 'topicmodels', 'wordcloud', 'ggplot2', 'lsa', 'GPArotation', 'RWeka', 'stringr', 'readtext')
if(! require(i, character.only = TRUE))
{install.packages(i, dependencies = TRUE)}
for(i in packages){
for(i in packages){if(! require(i, character.only = TRUE))
for(i in packages){
for(i in packages){
{install.packages(i, dependencies = TRUE)}}
pal <- brewer.pal(9, "set1")
getwd()
getwd()
getwd()
setwd("F://R Text Mining")
packages = c("tm", "KoNLP", "topicmodels", "NLP", "wordcloud", "gglpot2", "lsa", "GPArotation", "RWeka", "stringr", "readtext")
for(i in packages){
if(! require(i, character.only = TRUE))
{install.packages(i, dependencies = TRUE)}
}
install.packages("stm")
rm(list = ls())
pdf.options(family = "Korea1deb")
pal <- brewer.pal(9, "Set1")
folder = "./Institutional Theory"
text_r <- readtext(paste0(foler, "/*.pdf"))
getwd()
folder = "./Institutional Theory"
text_r <- readtext(paste0(folder, "/*.pdf"))
txt = text_r$text
ttt <- array(data = NA, dim = length(txt))
for(i in 1:length(txt)){
t1 <- txt[i]
t2 <- as.String(t1)
t3 <- gsub("\n", " ", t2)
ttt[i] <- t3
}
txt <- ttt
txt <- str_replace_all(txt, "@[[:graph:]]*", "")
txt <- str_replace_all(txt, "http://[[:graph:]]*", "")
txt <- str_replace_all(txt, "[^[:graph:]]", " ")
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("\n", " ", txt)
txt <- gsub("#", " ", txt)
txt <- gsub("\r", " ", txt)
txt <- gsub("RT", " ", txt)
txt <- gsub("http", " ", txt)
txt <- gsub("  ", " ", txt)
corpus <- Corpus(VectorSource(txt))
myStopwords <- c(stopwords("english"))
corpus <- tm_map(corpus, removeWords, myStopwords)
corpus <- tm_map(corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords)
corpus <- tm_map(corpus, removeNumbers)
uniTokenizer <- function(x) unlist(strsplit(as.character(x), "[[:space:]]+"))
control2 = list(tokenize = words, removeNumbers = TRUE, removePunctuation = TRUE, stopwords = TRUE)
tdm <- DocumentTermMatrix(corpus, control = control2)
TermFreq <- colSum(as.matrix(tdm))
TermFreq <- colSums(as.matrix(tdm))
summary(TermFreq)
TermFreq2 <- subset(TermFreq, TermFreq>300)
TermFreq2
library(topicmodels)
lda.out <- LDA(tdm, control = list(seed=11), k=5)
dim(lda.out@gamma)
dim(lda.out@beta)
terms(lda.out, 12)
posterior_lda <- posterior(lda,out)
posterior_lda <- posterior(lda.out)
round(posterior_lda$topics, 3)
lda.topics <- data.frame(posterior_lda$topics)
apply(lda.topics, 1, sum)
apply(lda.topics, 2, sum)
ctm.out <- CTM(tdm, control=list(seed=44), k=10)
